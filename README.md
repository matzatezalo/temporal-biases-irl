This repository presents my Bachelor thesis project titled "Investigating Inverse Reinforcement Learning from Human Behavior: Effect of Demonstrations with Temporal Biases on Learning Rewards using Inverse Reinforcement Learning."

This project explores the impact of temporal cognitive biases on Inverse Reinforcement Learning (IRL) algorithms when learning reward functions from expert demonstrations. Humans are prone to cognitive biases, including time-inconsistent decision-making, which can complicate the recovery of maximized rewards. The research proposes a methodology to generate synthetic demonstrations that emulate human data with these biases. The Maximum Entropy IRL (MEIRL) algorithm is utilized to recover reward functions from these biased expert models and compare their performance against unbiased models, based on the paper from (Ziebart et al., 2008).

While Inverse Reinforcement Learning (IRL) has various real-world applications, such as autonomous driving and genetic regulatory networks, challenges remain in adapting to human systematic biases. This research specifically investigates the extent to which IRL can learn rewards from demonstrations containing temporal cognitive biases.

The research employs a 6x6 state grid-world environment implemented in Python, inspired by existing work on MEIRL. This environment features different rewards located in terminal states. Four types of expert agents are implemented to generate demonstrations: Unbiased (Optimal) Agent (represents optimal behavior), Time Consistent (Present) Biased Agent (incorporates exponential discounting), and Time Inconsistent Biased Agents (Naive and Sophisticated) (utilize a hyperbolic discount function, reflecting how humans discount future rewards). The Naive Agent models its future self with current actions and values, while the Sophisticated Agent understands its future plans and acts accordingly to avoid temptation. Expert agents are trained through value iteration, adapted to include temporal biases. Stochastic policies are created from these value derivations and used to generate trajectories, forming the expert demonstrations. The Maximum Entropy IRL (MEIRL) algorithm is then used to recover the reward functions from these generated trajectories, chosen for its ability to handle imperfect behavior and sub-optimal expert demonstrations by finding the solution with maximum entropy.

The findings indicate that all implemented biases significantly affect reward learning. MEIRL can learn rewards from demonstrations with time-consistent bias, although not always achieving the highest degree of success, particularly with the recovery of smaller rewards. The similarity of trajectories between biased and optimal agents suggests that the discount factor has a nuanced impact on reward learning. For time-inconsistent agents, MEIRL did not learn rewards to a perfect extent compared to the unbiased agent. However, the performance did not vary greatly between time-consistent and time-inconsistent biases. The sophisticated agent showed slightly better performance in learning rewards compared to the time-consistent agent. The Naive agent's inconsistent behavior presumably contributed to a lower success rate in reward recovery compared to the sophisticated agent.

The key technologies used in this project include Python, Inverse Reinforcement Learning (IRL), Maximum Entropy Inverse Reinforcement Learning (MEIRL), Markov Decision Process (MDP), and Value Iteration.

Future work could involve further adapting the grid-world environment and using a recursive approach to construct time-inconsistent agents for more accurate behavior and improved reward learning, and incorporating more metrics to evaluate reward recovery performance for time-inconsistent agents.

This thesis was submitted to the EEMCS Faculty at Delft University of Technology. Supervisors were Luciano Cavalcante Siebert and Angelo Caregnato Neto. An electronic version of this thesis is available at http://repository.tudelft.nl/.
